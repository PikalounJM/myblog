---
title: Text Mining "Anti-discrimination law"
description: Text mining, Crawling, One-hot encoding, Wordcloud
category: projects
dateString: Oct 2022 - Dec 2022 
cover:
  image: "img/law.PNG"
  alt:
  caption:
  relative: true
showtoc: true
draft: false
weight: 204
---

### Coding
  The complete code can be found in 🔗 [Github](https://github.com/PikalounJM/Text-Mining/blob/main/Non-discrimination.ipynb)

### Backgroud
- As M.S's assignment project, analysis of public awareness by identifying trends in favor/opposition to the 'Anti-Discrimination law' using web crawling.

### Dataset & Tool
- Naver news and blog **using Python**

&nbsp;

### ✏️ Code Example

**1) 형태소 분석(Morphology Analysis)**

```python
import konlpy
from konlpy.tag import Okt
from konlpy.tag import Hannanum, Kkma, Komoran, Mecab

new_list = "".join(list)

#형태소 분석

twt = Okt()
sentence_tag = []
for sentence in list:
    morph = twt.pos(sentence)
    sentence_tag.append(morph)
    print(morph)
    print('-' * 30)

update = twt.morphs(new_list)

from collections import Counter
update1 = Counter(update)
print(update1)

#명사 빈도 분석
noun = twt.nouns(new_list)

from collections import Counter
noun1 = Counter(noun)
print(noun1)

#데이터 순위: 1위)차별금지법, 2위) 차별, 3위) 한국, 4위) 성소수자, 5위) 성적지향
```
&nbsp;

**2) 크롤링(Crawling)**

```python
#네이버에 '차별금지법' 검색 후, 뉴스 제목 및 URL 크롤링
import requests
from pandas import DataFrame
from bs4 import BeautifulSoup
import re
from datetime import datetime
import os

date = str(datetime.now())
date = date[:date.rfind(':')].replace(' ', '_')
date = date.replace(':','시') + '분'

query = '차별금지법'
news_num = 150
query = query.replace(' ', '+')


news_url = 'https://search.naver.com/search.naver?where=news&sm=tab_jum&query={}'

req = requests.get(news_url.format(query))
soup = BeautifulSoup(req.text, 'html.parser')


news_dict = {}
idx = 0
cur_page = 1

while idx < news_num:

    table = soup.find('ul',{'class' : 'list_news'})
    li_list = table.find_all('li', {'id': re.compile('sp_nws.*')})
    area_list = [li.find('div', {'class' : 'news_area'}) for li in li_list]
    a_list = [area.find('a', {'class' : 'news_tit'}) for area in area_list]

    for n in a_list[:min(len(a_list), news_num-idx)]:
        news_dict[idx] = {'title' : n.get('title'),
                          'url' : n.get('href')}
        idx += 1

    cur_page += 1

    pages = soup.find('div', {'class' : 'sc_page_inner'})
    next_page_url = [p for p in pages.find_all('a') if p.text == str(cur_page)][0].get('href')

    req = requests.get('https://search.naver.com/search.naver' + next_page_url)
    soup = BeautifulSoup(req.text, 'html.parser')

news_df = DataFrame(news_dict).T
```
&nbsp;

**3) 원-핫 인코딩(One-hot encoding)**

```python
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.cluster import KMeans
from pyclustering.cluster import kmedoids
import numpy as np

vec = CountVectorizer()
X = vec.fit_transform(document)

df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())
```
![One-hot](/img/onehot.PNG)

&nbsp;

**4) 워드클라우드(Wordcloud)**

```python
from wordcloud import WordCloud #토끼 이미지로 워드클라우드 시각화하기
from collections import Counter
import matplotlib.pyplot as plt
from PIL import Image
import numpy as np

font_path = 'HMFMMUEX.TTC'

animal_mask = np.array(Image.open("animal.png"))

wordcloud = WordCloud(
    font_path = font_path,
    width = 800,
    height = 800,
    background_color="white",
    mask = animal_mask)

count = Counter(word_list)
wordcloud = wordcloud.generate_from_frequencies(count)
plt.figure(figsize=(10, 10))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()
```
&nbsp;

**5) 토픽 모델링(Topic modeling)**

```python
from gensim.models import CoherenceModel

print('\nPerplexity: ', ldamodel.log_perplexity(corpus)) #복잡도 계산하고 그 안에 단어들로 구성된 topic안에서 응집성이 어느정도인지 scoring
coherence_model_lda = CoherenceModel(model=ldamodel, texts=texts, dictionary=dictionary,topn=10)
coherence_lda = coherence_model_lda.get_coherence()
print('\nCoherence Score: ', coherence_lda) #응집도, 응집도가 낮을경우에는 쓰지 않는것이 좋다.
```

Perplexity:  -7.837716129815166

Coherence Score:  0.6466521187737081

**✏️ Graph**

```python
import matplotlib.pyplot as plt
perplexity_values = []
for i in range(2,10):
    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=i, id2word = dictionary)
    perplexity_values.append(ldamodel.log_perplexity(corpus))

x = range(2,10)
plt.plot(x, perplexity_values)
plt.xlabel("Number of topics")
plt.ylabel("Perplexity score")
plt.show() #5개 이상이 넘어가면 복잡도가 떨어짐

coherence_values = []
for i in range(2,10):
    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=i, id2word = dictionary)
    coherence_model_lda = CoherenceModel(model=ldamodel, texts=texts, dictionary=dictionary,topn=10)
    coherence_lda = coherence_model_lda.get_coherence()
    coherence_values.append(coherence_lda)

x = range(2,10)
plt.plot(x, coherence_values)
plt.xlabel("Number of topics")
plt.ylabel("coherence score")
plt.show() #5개를 뽑았을때가 응집도 숫자가 가장 높음. 응집력이 가장 높을 때와 Perplexity가 높지 않을때가 가장 적절함
```

